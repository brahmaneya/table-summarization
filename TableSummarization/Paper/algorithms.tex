%!TEX root = TableSummarization.tex


\section{Smart Drill-Down Algorithms} \label{sec:algorithms}
We now describe online algorithms for implementing
the smart drill-down operator. We assume in this section that all columns are categorical (so numerical columns have been bucketized beforehand). We further discuss bucketization of numerical attributes in the Extensions section in the technical report~\cite{tr}.


\subsection{Problem Reduction and Important Property} \label{sec:reduction}
When the user drills down on a rule $r^{\prime}$, we want to find the highest scoring list of rules to expand rule $r^{\prime}$ into. If the user had clicked on a $\star$ in a column $c$, then we have the additional restriction that all resulting rules must have a non-$\star$ value in column $c$. We can reduce Problem~\ref{prob:optimal-subrule-list} to the following simpler problem by removing the user-interaction based constraints: 

\begin{problem}\label{prob:optimal-rule-list}
Given a table $T$, a monotonic weight function $W$, and a number $k$, to find the list $R$ of $k$ rules that maximizes the total score given by :
$$\text{Score}(R) = \sum_{r \in R}W(r)MCount(r,R)$$
\end{problem}

\noindent Problem~\ref{prob:optimal-subrule-list} with parameters $(T, W, k)$ can be reduced to Problem~\ref{prob:optimal-rule-list} as follows:
\begin{enumerate}
\item $[$Rule Drill-Down$]$ If the user clicked on rule $r$ in Problem~\ref{prob:optimal-subrule-list}, then we can conceptually make one pass through the table $T$ to filter for tuples covered by rule $r$, and store them in a temporary table $T_r$. Then, we solve Problem~\ref{prob:optimal-rule-list} for parameters $(T_r, W, k)$.
\item $[$Star Drill-Down$]$ If the user clicked on a $\star$ in column $c$ of rule $r$, then we first filter table $T$ to get a smaller table $T_r$ consisting of tuples from $T$ that are covered by $r$. In addition, we change the weight function $W$ from Problem~\ref{prob:optimal-subrule-list} to a weight function $W^{\prime}$ such that : For any rule $r^{\prime}$, $W^{\prime}(r^{\prime}) = 0$ if $r^{\prime}$ has a $\star$ in column $c$, and $W^{\prime}(r^{\prime}) = W(r^{\prime})$ otherwise. Then, we solve Problem~\ref{prob:optimal-rule-list} for parameters $(T_r, W^{\prime}, k)$.
\end{enumerate}


As a first step towards solving Problem~\ref{prob:optimal-rule-list}, we show that the rules in the optimal list must effectively be ordered in decreasing order by weight. Note that the weight of a rule is independent of its $MCount$. The $MCount$ of a rule is the number of tuples that have been `assigned' to it, and each tuple assigned to rule $r$ contributes $W(r)$ to the total score. Thus if the rules are not in decreasing order by weight in a rule list $R$, then switching the order of rules in $R$ transfers some tuples from a lower weight rule to a higher weight rule, which can increase total score.

\begin{lemma}\label{lemma:rule-ordering}
Let $R$ be a rule-list. Let $R^{\prime}$ be the rule-list having the same rules as $R$, but ordered in descending order by weight. Then
$$\text{Score}(R^{\prime}) \geq \text{Score}(R)$$
\end{lemma}
The proof of this lemma, as well as other proofs, can be found in the appendix of the technical report~\cite{tr}. 

\noindent Thus it is sufficient to restrict our attention to rule-lists that have rules sorted in decreasing order by weight. Or equivalently, we can define Score for a \emph{set} of rules as follows:

\begin{definition}\label{def:set-score}
Let $R$ be a set of rules. Then the Score of $R$ is defined by
$$\text{Score}(R) = \text{Score}(R^{\prime})$$
where $R^{\prime}$ is the list of rules obtained by ordering the rules in $R$ in decreasing order by weight.
\end{definition}

This gives us a reduced version of Problem~\ref{prob:optimal-rule-list}: 
\begin{problem}\label{prob:optimal-rule-set}
Given a table $T$, a monotonic weight function $W$, and a number $k$, find the set (not list) $R$ of $k$ rules which maximizes Score($R$) as defined in Definition~\ref{def:set-score}.
\end{problem}

\noindent The reduction from Problem~\ref{prob:optimal-rule-list} to Problem~\ref{prob:optimal-rule-set} is clear. We now first show that Problem~\ref{prob:optimal-rule-set}, and consequently Problem~\ref{prob:optimal-subrule-list} and Problem~\ref{prob:optimal-rule-list} are NP-Hard, and then present an approximation algorithm for solving Problem~\ref{prob:optimal-rule-set}.

\subsection{NP-Hardness for Problem~\ref{prob:optimal-rule-set}}
We reduce the well known NP-Hard {\em Maximum Coverage Problem} to a special case of Problem~\ref{prob:optimal-rule-set};
thus demonstrating the NP-Hardness of Problem~\ref{prob:optimal-rule-set}. The Maximum Coverage Problem is as follows: 
\begin{problem}\label{prob:maximum-coverage}
Given a universe set $U$, an integer $k$, and a set $S = \left\lbrace S_1, S_2, ... S_m \right\rbrace$ of subsets of $U$ (so each $S_i \subset U$), find $S^{\prime} \subset S$ such that $|S^{\prime}| = k$, which maximizes $\text{Coverage}(S^{\prime}) = |\bigcup_{s \in S^{\prime}} s|$.
\end{problem}
Thus, the goal of the maximum coverage problem is to find a set of $k$ of the given subsets of $U$ whose union `covers' as much of $U$ as possible. We can reduce an instance of the Maximum Coverage Problem (with parameters $U, k, S$) to an instance of Problem~\ref{prob:optimal-rule-set}, which gives us the following lemma:
\begin{lemma}
Problem~\ref{prob:optimal-rule-set} is NP-Hard
\end{lemma}

\subsection{Algorithm Overview}\label{sec:alg-overview}
Given that the problem is NP-Hard, we now present our algorithms 
for approximating the solution to Problem~\ref{prob:optimal-rule-set}. 
The problem consists of finding a set of given size $k$, that maximizes 
Score. 


The next few sections fully develop the
details of our solution:

\squishlist
\item We show that the Score function is {\em submodular}, and hence an approximately optimal set can be obtained using a greedy algorithm. At a high level, this greedy algorithm is simple to state. The algorithm runs for $k$ steps;
we start with an empty rule set $R$, and then at each step, we add the next best rule that maximizes Score
 
\item In order to find the rule $r$ to add in each step, we need to measure the impact on Score for each $r$. This is done in several passes over the table, using ideas from the a-priori algorithm~\cite{apriori} for frequent item-set mining. 
\squishend

In some cases, the dataset may still be too large for us to return a good rule set in
a reasonable time; in such cases, we may want to run our algorithm on a sample of the table
rather than the entire table. In Section~\ref{sec:sampling}, we describe a scheme 
for maintaining multiple samples in memory and using them to improve response 
time for different drill down operations performed by the user. Our sampling scheme dynamically adapts to the current interaction scenario that the user is in; drawing from ideas in approximation algorithms and optimization theory.

\subsection{Greedy Approximation Algorithm}\label{sec:greedy-approx}
\stitle{Submodularity:} We will now show that the Score function over sets of rules has a property called {\em submodularity}, giving us a greedy approximation algorithm for optimizing it. 
\begin{definition}
A function $f: 2^S \rightarrow \mathbb{R}$ for any set $S$ is said to be submodular if and only if, for every $s \in S$, and $A \subset B \subset S$ with $s \notin A$:
$$f(A \cup \left\lbrace s \right\rbrace) - f(A) \geq f(B \cup \left\lbrace s \right\rbrace) - f(B)$$
\end{definition}
Intuitively, this means that the marginal value of adding an element to a set $S$ cannot increase if we add it to a superset of $S$ instead. For monotonic non-negative submodular functions, the problem of finding the set of a given size with maximum value for the function can be found approximately in a greedy fashion. 

\begin{lemma}\label{lemma:submodular}
For a given table $T$, the Score function over sets $S$ of rules, defined by :
$$\text{Score}(S) = \sum_{r \in S} MCount(r,S)W(r)$$
is submodular.
\end{lemma}

%TODO: Give  better name to `Greedy Algorithm', and use that name to reference in later as well, in sampling/experimental/extension sections. 
\stitle{High-Level Procedure:} Now, based on the submodularity property, the greedy procedure,
as listed below, has desirable approximation guarantees:
\begin{framed}
\vspace{-15pt}
\begin{enumerate}
\item Set $S = \phi$
\item For $i$ from $1$ to $k$
\begin{enumerate}
\item Find the rule $r$ for which Score($S \cup \left\lbrace r \right\rbrace$) is the highest.
\item $S = S \cup \left\lbrace r \right\rbrace$
\end{enumerate}
\end{enumerate}
\vspace{-15pt}
\end{framed}
Since Score is a submodular function of the set $S$, 
this greedy procedure is guaranteed to give us a score within a $1 - \frac{1}{e}$ factor of the optimum. 

The expensive step in the above procedure is the step where the Score is computed for every
single rule. Given the number of rules can be very large, this can be an especially time-consuming process.

Instead of using the procedure described above directly, we instead develop a ``parameterized'' version 
that will admit further approximation (depending on the parameter) in order to reduce computation further. We describe
this algorithm next.

\stitle{Parametrized Algorithm:} Our algorithm pseudo-code is given in the box labelled Algorithm~\ref{algo:best-rule-set}. The algorithm takes four parameters as input: the table $T$, the number $k$ of rules required in the final solution list, a parameter $m_w$ (which we describe in the next paragraph), and the weight function $W$. 

The parameter $m_w$ stands for \textit{Max Weight}. The parameter $m_w$ tells the algorithm to assume that all rules that get selected in the optimal solution are going to have weight $\leq m_w$. Thus, if $S_o$ denotes set of rules with maximum score, then as long as $m_w \geq \textrm{max}_{r \in S_o}W(r)$, Algorithm~\ref{algo:best-rule-set} is guaranteed to return $S_o$. On the other hand if $m_w < W(r)$ for some $r \in S_o$, then there is a chance that the set returned by Algorithm~\ref{algo:best-rule-set} does not contain $r$. Algorithm~\ref{algo:best-rule-set} runs faster for smaller values of $m_w$, and may only return a suboptimal result if $m_w < \textrm{max}_{r \in S_o}W(r)$. In practice, $\textrm{max}_{r \in S_o}W(r)$ is usually small. This is because as the size (and weight) of a rule increases, its Count falls rapidly. The Count tends to decrease exponentially with rule size, while Weight increases linearly for common weight functions (such as $W(r) = \text{Size}(r)$). Thus, rules with high weight and size have very low count, and are unlikely to occur in the optimal solution set $S_o$. Our experiments in Section~\ref{sec:experiments} also show that the weights of rules in the optimal set tend to be small.

% TODO: scheme/heuristic for finding good m_w values, since it seems arbitrary. Later, experiments testing effect of m_w on running time, and experiments to test finding heuristic, and show that values are in fact small.

Algorithm~\ref{algo:best-rule-set} initializes the solution set $S$ to be empty, and then iterates for $k$ steps, adding the best marginal rule at each step. To find the best marginal rule, it calls a function to find the best marginal rule given the existing set of rules $S$. 

\stitle{Finding the Best Marginal Rule:} In order to find the best marginal rule, we need to find the marginal values of several rules and then choose the best one. A brute-force way to do this would be to enumerate all possible rules, and to find the marginal value for each of those rules in a single pass over the data. But the number of possible rules may be almost as large as the size of the table itself, making this step very expensive in terms of computation and memory. 

In order to avoid counting too many rules, we use an idea from the {\em a-priori} algorithm for frequent itemset mining~\cite{apriori}. Recall that the a-priori algorithm is used to find all frequent itemsets that have a support greater than a threshold. Unlike the frequent itemset mining algorithm, our goal is to find the single best marginal rule. Since we only aim to find one rule at a time, our pruning power is significantly higher than a vanilla a-priori algorithm, and we terminate in much fewer passes over the dataset. 
We compute the best marginal rule over several passes, with the maximum number of passes equal to the maximum size of a rule. In the $j^{th}$ pass, we compute counts and marginal values for rules of size $j$. To give an example, suppose we had three columns $c_1$, $c_2$, and $c_3$. In the first pass, we would compute the counts and marginal values of all rules of size $1$. In the second pass, instead of finding marginal values for all size $2$ rules, we can use our knowledge of counts from the first pass to upper bound the potential counts and marginal values of size $2$ rules, and be more selective about which rules to count in the second pass. For instance, suppose we know that the rule $(a, \star, \star)$ has a count of $1000$, while $(\star, b, \star)$ has a count of $100$. Then for any value $c$ in column $c_3$ we would know that the count of $(\star, b, c)$ is at most $100$ because it cannot exceed that of $(\star, b, \star)$. This implies that the maximum marginal value of any super-rule of $(\star, b, c)$ having weight $\leq m_w$ is at most $100m_w$. If the rule $(a, \star, \star)$ has a marginal value of $800$, then the marginal value of any super-rule of $(\star, b, \star)$ cannot possibly exceed that of $(a, \star, \star)$. Since our aim is to only find the highest marginal value rule, we can skip counting for all super-rules of $(\star, b, \star)$ for future passes.

We now describe the function to find the best marginal rule. The pseudo-code for the function is in the box titled Algorithm~\ref{algo:best-marginal-rule}. The function maintains a threshold $H$, which is the highest marginal value that has been found for any rule so far. The function makes several iterations (Step $3$), counting marginal values for size $j$ rules in the $j^{th}$ iteration. We maintain three sets of rules. $C$ is the set of all rules whose marginal values have been counted in all previous iterations. $C_n$ is the set of rules whose marginal values are going to be counted in the current pass. And $C_o$ is the set of rules whose marginal values were counted in the previous iteration. For the first pass, we set $C_n$ to be all rules of size $1$. Then we compute marginal values for those rules, and set $C = C_o = C_n$.

For the second pass onwards, we are more selective about which rules to consider for marginal value evaluation. We first set $C_n$ to be the set of rules of size $j$ which are super-rules of rules from $C_o$. Then for each rule $r$ from $C_n$, we consider the known marginal values of its sub-rules from $C$, and use them to upper-bound the marginal value of all super-rules of $r$, as shown in Step 3.3.2. Then we delete from $C_n$ the rules whose marginal value upper bound is less than the currently known best marginal value, since they have no chance of being returned as the best marginal rule. Then we make as actual pass through the table to compute the marginal value of the rules in $C_n$, as shown in Step 3.5. If in any round, the $C_n$ obtained after deleting rules is empty, then we terminate the algorithm and return the highest value rule. 

Note that the reader may be wondering why we did not simply count the score of each rule using a variant of the a-priori algorithm in one pass, and then pick the set of rules that maximizes score subsequently. This is because doing so will lead to a sub-optimal set of rules: by not accounting for the rules that have already been selected, we will not be able to ascertain the marginal benefit of adding an additional rule correctly.

\begin{algorithm}\label{algo:best-rule-set}
\KwIn{$k$ (Number of rules required), $T$ (database table), $m_w$ (max weight), $W$ (weight function)}
\KwOut{$S$ (Solution set of rules)}
$S = \phi$ 

\For {$i$ from $1$ to $k$}{
$R_m = \text{Find\_best\_marginal\_rule}(S, T, m_w, W)$ \tcc*{Calling Algorithm~\ref{algo:best-marginal-rule}}

$S = S \cup \left\lbrace R_m \right\rbrace$
}
\Return $S$
\caption{Greedy Algorithm for Problem~\ref{prob:optimal-rule-set}}
\end{algorithm}


\begin{algorithm}\label{algo:best-marginal-rule}
\KwIn{$S$ (Current solution set), $T$ (database table), $m_w$ (max weight), $W$ (weight function)}
\KwOut{$R_m$ (Rule which adds the highest marginal value among rules with weight $\leq m_w$)}
$H = 0$ \tcc*{Threshold for deciding if to find count for a rule.}
$C = C_o = C_n = \phi$ \tcc*{Set of all, old and new candidate rules respectively. }
\For{$j$ from $1$ to number of columns in $T$}{
\If {$j = 1$}{
$C_n = \text{ all rules of size } 1$
}
\Else{ 
$C_n = \text{all size-}i$ super-rules of rules from $C_o$
}
\ForEach {$R \in C_n$}{
$M =\infty$ \tcc*{Upper bound on marginal value count of $R$}
\ForEach {$R$-sub-rule $R^{\prime} \in C$}{  
$M = \text{min}(M, \text{MarginalVal}(R^{\prime}) + \text{Count}(R^{\prime})(m_w - W(R^{\prime}))$
}
\If {$(M < H)$}{ 
$C_n = C_n \setminus \left\lbrace R \right\rbrace$ \tcc{Delete $R$ if its max count is too small for $R$ to possibly be in the solution}
}
}
\If {$C_n = \phi$}{
break;
}
\ForEach {$R \in C_n$}{
Count$(R) = 0$ \tcc*{Initialize}
MarginalValue$(R) = 0$ \tcc*{Initialize}
}
\ForEach {$t \in T$}{
Let $R_S$ be the highest weight rule in $S$ that covers $t$
\ForEach {$R \in C_n$ that covers $t$}{
Count($R$) $++$ 

MarginalValue($R$) $+=$ $W(R) - \text{min}(W(R), W(R_S))$
}
}
$C = C \cup C_n$

$C_o = C_n$

$C_n = \phi$


$H = \textrm{max}_{R \in C}(\text{MarginalValue}(R))$
}
\Return $\textrm{argmax}_{r \in C} \text{MarginalValue}(r)$
\caption{Find best marginal rule}
\end{algorithm}


\subsection{Dynamic Sampling for Large Tables}\label{sec:sampling}
We now describe our sampling schemes for improving the running time of our algorithm on tables that are too large to fit in main memory. First in Section~\ref{sec:sample-using}, we motivate the need for sampling. Then we describe a component of our system, called the {\em SampleHandler}, which is responsible for creating and maintaining samples of the table in memory, subject to user specified memory constraints. The SampleHandler maintains multiple samples corresponding to different parts of the table, which can be used depending on which rule the user decides to expand next. Then in Section~\ref{sec:sampling_algorithms}, we describe ways to efficiently allocate memory to different samples, so as to maximize the probability that we can respond to the next user operation without accessing the hard disk. Finally, we mention some additional optimizations we can make, and heuristics for setting the minimum sample size required from the SampleHandler.

\subsubsection{Using samples to respond to user operations}\label{sec:sample-using}
Our greedy algorithm needs to make multiple passes over the entire table in order to find counts of rules. These passes can be very expensive if the table is large, especially if the table does not fit in main memory. If we want exact counts for rules, we have no choice but to read the entire table. 

But if we are willing to accept approximate counts rather than exact counts for rules, we can speed up our algorithm by loading a sample of the table into main memory, finding rule counts on the sample, and scaling up the count. Thankfully since our goal is to find a representative coverage of the table,
if we miss out on a few rare tuples, it does not hurt us. If we had obtained the sample by sampling each tuple with probability $p$, then we must scale up the sample count of each rule by $\frac{1}{p}$ to get an estimate of its count over the full table. 

Thus, we use sampling to trade-off a small amount of accuracy for a faster response time. Our system includes a {\em SampleHandler}, which is given a certain memory capacity $M$, and a minimum sample size $minSS$ (both specified by the user).
The $minSS$ parameter is the minimum number of sample tuples that are used to determine counts while running our greedy algorithm, and this parameter determines how accurate our count estimates will be. We later provide a way to find reasonable values of $minSS$.

At all points, the SampleHandler maintains a set of samples in memory. For instance, it may keep a sample of tuples used to expand the first (trivial) rule, and another sample used to expand the rule last clicked on by the user. Each sample $s$ is an object with three attributes: A `filter' rule $f_s$, a scaling factor $N_s$ and a set $T_s$ of tuples from the table. The set $T_s$ consists of a $\frac{1}{N_s}$ uniformly sampled fraction of tuples covered by $f_s$. The scaling factor $N_s$ is used to translate the count of a rule on the sample into an estimate of the count over the entire table. The sum of $|T_s|$ over all samples $s$ is not allowed to exceed capacity $M$ at any point. 
% TODO: Give example of what samples we'd keep.

Whenever the user drills down on a rule $r$, our system calls the SampleHandler with argument $r$, which returns a sample $s$ whose filter value is given by $f_s = r$ and has $|T_s| \geq minSS$. Thus the $T_s$ of the returned sample consists of a uniformly random set of tuples covered by $r$. The SampleHandler also computes $N_s$ when a sample is created. Then we run Algorithm~\ref{algo:best-rule-set} on sample $s$ (with a modified weight function in case the user clicked on a $\star$) to obtain the list of rules to display. The counts of the rules on the sample are multiplied by $N_s$ before being displayed, to get estimated counts on the entire table. In addition since the sample is uniformly random, we can also compute confidence intervals on the estimated count of each displayed rule, although we do not currently display the confidence intervals.

When the SamplerHandler gets called with argument $r$, it needs to find or create a sample with $r$ as the filter rule. At the beginning when it gets called with the empty rule as an argument, there are no samples in memory and it must make a pass through the data to generate a sample. Creating a new sample by making a pass through the table is called \textbf{Create} (further described below). At later stages, when there are potentially multiple samples available, there are multiple ways it could return a sample for rule $r$:
\begin{enumerate}
\item \textbf{Find:} If the SampleHandler finds an existing sample $s$ in memory, which has $r$ as its filter rule (i.e. $f_s = r$) and at least $minSS$ tuples ($|T_s| \geq minSS$, then it simply returns sample $s$. Algorithm~\ref{algo:best-rule-set} can then be run on $s$. 

\item \textbf{Combine:} If \textbf{Find} doesn't work i.e. if the SampleHandler cannot find an existing sample with filter $r$ and $\geq minSS$ tuples, then it looks at all existing samples $s^{\prime}$ such that $f_{s^{\prime}}$ is a sub-rule of $r$. If the set of all tuples that are covered by $r$, from all such $T_{s^{\prime}}$'s combined, exceeds $minSS$ in size, then we can simply treat that set as our sample for rule $r$.
We can show that tuples that are covered by $r$, from the combination of $T_{s^{\prime}}$s, follow a uniform distribution. That is, each table tuple $t$ that is covered by $r$ is equally likely to appear in a $T_{s^{\prime}}$. 

Note that the \textbf{Combine} procedure doesn't really require additional memory apart from the temporary memory used by Algorithm~\ref{algo:best-rule-set}. Since all the tuples in the `new' sample are already present in existing samples, it can give Algorithm~\ref{algo:best-rule-set} a set of temporary pointers to the tuples, and the memory for the pointers can be freed as soon as the sample has been processed by Algorithm~\ref{algo:best-rule-set}. In contrast, if we had created a new sample from hard disk, we would maintain the sample even after Algorithm~\ref{algo:best-rule-set} terminated, and would hence need to use memory from the SampleHandler's capacity $M$.
%TODO: Not sure how clear/necessary this is.  

\item \textbf{Create:} If \textbf{Combine} doesn't work either, then the SampleHandler needs to create a new sample $s$ with $f_s = r$ by making a pass through the table. Making a pass can be expensive for big tables, so we only use \textbf{Create} when \textbf{Find} and \textbf{Combine} cannot be used. In addition, creating a new sample requires memory, and since memory capacity is limited (given by parameter $M$), it may necessitate shrinking or deleting some existing samples. 
We can use reservoir sampling~\cite{maibdr1983,Vitter:1985:RSR:3147.3165} to get a uniformly random sample of given size in a single pass through the table. 

In addition, the SampleHandler need to create a sample of size exactly equal to $minSS$. It can create a sample of $< minSS$ size if there are some tuples covered by $r$ present in existing samples, or $> minSS$ (if enough memory is available). Making a larger sample is advantageous not only to get higher accuracy, but also because, when the user later drills down on a sub-rule $r^{\prime}$ of $r$, having a large $r$ sample increases the chance that the \textbf{Combine} strategy will work for $r^{\prime}$, which can let us avoid making another expensive pass through the table. For example, if $minSS = 500$, but we get a size $2000$ sample $s$ for the empty rule, then when the user clicks on one of its sub-rules, say $r$, there is a good chance the $2000$ tuples from $T_s$ contain at least $500$ tuples covered by $r$ and that allows us to display the rule-list expanding $r$ quickly instead of making another pass through the table. We describe our algorithms for deciding what sized samples to create, and what samples to shrink, in Section~\ref{sec:sampling_algorithms}.

\item \textbf{Delete/Shrink:} When the SampleHandler uses \textbf{Create}, it may not have enough memory left to create a new sample. In that case, the SampleHandler needs to delete or shrink an existing sample. It can use a heuristic, like a LRU (Least-Recently-Used) policy to decide which sample to delete or shrink. Note that shrinking a sample by randomly deleting some tuples (even if the sample size goes below $minSS$) may be better than deleting it entirely, because in future, we may be able to recreate a sample using \textbf{Combine} on the existing shrinked sample along with other samples.
%For shrink, prefer shrinking samples of the largest size (> minSS) first?
\end{enumerate}

We describe our algorithm for deciding what samples to shrink, and what sized samples to create, in Section~\ref{sec:sampling_algorithms}. 

\textbf{Pre-fetching:} When the user clicks on rule $r$ (on the rule itself or on a $\star$ in the rule), we need to get a sample, run the Algorithm~\ref{algo:best-rule-set}, and display a rule-list to the user. If we use \textbf{Find} or \textbf{Combine}, then we can display the rule-list much faster because we don't have to read the entire table. But after expanding $r$, there is a high chance that the user goes further and drills down on one of the sub-rules $r^{\prime}$ of $r$. We may not be able to use \textbf{Find} or \textbf{Combine} on $r^{\prime}$ with the existing samples. So while the user is reading the current rule-list obtained from drilling down on $r$, we can start making a pass through the table to create a bigger sample of $r$ in the background. That way, when the user expands $r^{\prime}$, some of the newly loaded tuples for $r$ will also be covered by $r^{\prime}$, increasing the chance that we can use \textbf{Combine} on $r^{\prime}$, and reduce our response time.
In addition, while we are making the pass in the background, we can find the exact counts for currently displayed rules (which only have estimated counts shown), and update them when our pass is complete.

\subsubsection{Algorithms for deciding what to sample}\label{sec:sampling_algorithms}
We now discuss algorithms for the SampleHandler to decide what sizes of Samples to maintain in memory. Briefly, we want to determine sample sizes that maximize the probability that the next user click can be answered using the existing samples, without reading from the hard disk. 

Suppose at some stage, we have a tree $U$ of rules displayed to the user. The root of the tree is the trivial rule, and the parent of each rule $r$ in the tree is the rule that was drilled down on that caused $r$ to be displayed. Internal nodes of $U$ are rules that have been expanded (drilled down on), while leaves are rules that have not been expanded. Let $L$ be the set of leaves. Each leaf is something that the user can potentially expand in the next step, and thus we would like to have pre-fetched samples for leaves.

We assume that we have a probability distribution over leaves, which assigns a probability that each leaf will be the next one to be expanded. In the absence of additional information, we can assume a uniform probability distribution. That is, we can assume that every leaf is equally likely to be expanded next. However, we can also use Machine Learning on past user behaviour, along with rule features such as `rule depth in tree' and `rule distance from last expanded rule' to get a better probability estimate of each rule being expanded. 

%TODO: Comment 15 of Aditya. I don't quite understand it. 
When the SamplerHandler uses \textbf{Create} for a rule $r$, it needs to make a pass through the entire table, as well as potentially shrink some existing samples to free up memory. Since making a pass over the the table from the hard disk is usually a bottleneck, it can also do things like creating samples for rules other than $r$, and augmenting existing samples, in the same pass. Hence, we assume that in a \textbf{Create} phase, the SampleHandler not only creates one new sample for $r$, but also potentially creates other new samples, or resizes existing samples. For each displayed rule $r^{\prime}$, it may create a new sample for $r^{\prime}$ in the same pass through the table. Based on the current leaves and their expansion probabilities, it finds an optimal set of sizes of samples to create for each displayed rule. More specifically, for every displayed rule $r^{\prime}$, it determines an integer $n_{r^{\prime}}$, and then creates a fresh sample $s_{r^{\prime}}$ with $f_{s_{r^{\prime}}} = r^{\prime}$ and $|T_{s_{r^{\prime}}}| = n_{r^{\prime}}$ while making its pass through the table. 

We choose the $n_{r^{\prime}}$ values so as to maximize the probability that the next user drill down can be satisfied using samples available in memory. We define some terms required to formalise this problem next. 

Let the `selectivity' of a rule be the fraction of tuples in $T$ that are covered by the rule. For each pair of rules $r_1, r_2 \in U$ such that $r_1$ is a sub-rule of $r_2$, we can estimate the ratio of selectivities of $r_1$ and $r_2$ using existing samples. Call this quantity $S(r_1, r_2)$. We define $S(r_1, r_2)$ to be $0$ if $r_1$ is not a sub-rule of $r_2$. Then if we have an $n_r$ sized sample with filter $r$ for each $r \in U$, the expected number of tuples covered by $r^{\prime} \in L$ is given by $$\sum_{r \in U} S(r, r^{\prime})n_r$$ For any $r^{\prime}$, define the function {\em ess} (for `effective sample size') as below: 
\begin{definition}\label{def:ess}
$$ess(r^{\prime}) = \sum_{r \in U} S(r, r^{\prime})n_r$$ 
\end{definition}
If $ess(r^{\prime}) \geq minSS$ then if the user expands $r^{\prime}$, we display the next rule list using \textbf{Find} or \textbf{Combine}, instead of having to make another pass through the table. We wish to maximize the probability that we can respond to the next user expansion without making another pass. We now formally define our problem below:
\begin{problem}\label{prob:sample-sizes}
Given a tree of rules $U$ with leaves $L$, a probability distribution $p$ over $L$, an integer `capacity', and selectivity ratio $S(r_1, r_2)$ for each $r_1, r_2 \in U$, 
choose an integer $n_r \geq 0$ for each $r \in U$ so as to $\textrm{maximize}$ :
$$\sum_{r^{\prime} \in L} p_{r^{\prime}}I_{ess(r^{\prime}) \geq minSS}$$
where the $I$'s are indicator variables, subject to :
$$\sum_{r \in U} n_r < \text{capacity}$$
\end{problem}
Problem~\ref{prob:sample-sizes} is non-linear and non-convex because of the indicator variables. If the tree $U$ is not too big, then it might be feasible to use an exponential algorithm to solve the problem. 

Otherwise, with an additional simplification, we can reduce it to the knapsack-like problem, and use a PTAS (Polynomial Time Approximation Scheme) to find the approximately optimal solution. The simplification is: For each $r \in L$, we assume that it will get tuples only from samples obtained for itself (filter $= r$) and its immediate parent. That is, we set $S(r_1, r_2)$ to be zero if $r_1 \neq r_2$ and $r_2$ is not a child of $r_1$. $ess(r)$ is redefined according to the new value of $S$ as well. So $$ess(r^{\prime}) = n_{r^{\prime}} + n_rS(r, r^{\prime})$$ where $r$ is the parent of $r^{\prime}$. 

Now consider a rule in $r_0 \in U \setminus L$ along with all its children. Let $M_{r_0}$ denote the set containing $r_0$ and all its leaf children. Then by our simplification, the number of tuples $n_{r^{\prime}}$ for any rule in $r^{\prime} \in M_{r_0}$ only affects the ess value of rules in $M_{r_0}$. This allows us to effectively split the problem into multiple subproblems, one per $M_{r_0}$. Thus for each non-leaf rule $r_0$ and all its children, we compute all `locally optimal' assignments of $n_r \mid r \in M_{r_0}$. Locally optimal means that we cannot get a higher value of `probability value' $\sum_{r \in M_{r_0}} p_rI_{ess(r) \geq minSS}$ for the same `sampling cost' $\sum_{r\in M_{r_0}} n_r$. Then we can use dynamic programming to combine the locally optimal solutions of different $M_{r_0}$s. We describe both these steps in detail below:

Let $r_0 \in U \setminus L$. Let $d$ be the number of leaf children of $r_0$. Let the children be $r_1, r_2, ... r_d$. For any child $r_i$, $n_{r_i}$ only contributes to it's own $ess$, whereas $n_{r_0}$ contributes to the $ess$ of all children $r_1, ... r_d$. Given a value of $n_{r_0}$, in a locally optimal solution, each child $r_i$ must satisfy:
\squishlist
\item If $n_{r_0}S(r_0, r_i) \geq minSS$, then $n_{r_i} = 0$ because otherwise, decreasing $n_{r_i}$ to $0$ would lower its sampling cost without improving its probability score. 
\item If $n_{r_0}S(r_0, r_i) < minSS$, then either $n_{r_i} = 0$ or $n_{r_i} = minSS - n_{r_0}S(r_0, r_i)$. This is because if $n_{r_i}$ is between $0$ and $minSS - n_{r_0}S(r_0, r_i)$, then we can decrease it to $0$, and if it is $> minSS - n_{r_0}S(r_0, r_i)$, then we can decrease it to $minSS - n_{r_0}S(r_0, r_i)$. Both these decreases would decrease sampling cost without affecting probability score. 
\squishend
Thus there are three kinds of children $r_i$: Those with $ess \geq minSS$ but $n_{r_i} = 0$, those with $ess < minSS$ and $n_{r_i} = 0$, and those with $ess \geq minSS$ and $n_{r_i} = minSS - n_{r_0}S(r_0, r_i)$. There are $3^d$ ways to assign each child to one of these categories, and each of those potentially gives us one locally optimal solution. Consider any such locally optimal solution $e$. For $e$ let children $r_{i_1}, r_{i_2}, ... r_{i_m}$ be in the first category, $r_{i_{m+1}}, .. r_{i_M}$ be in the second category, and $r_{i_{M+1}}, .. r_{i_d}$ in the third.  Then the `probability value' of solution $e$ is given by : 
$$P(e) = \sum_{j = 1}^{i_M}p_{j}$$
and its `Sampling Cost' is
$$S(e) = \frac{minSS}{S(r_0, r_{i_m})} + \sum_{j=i_m+1}^{i_M} minSS - \frac{minSS}{S(r_0, r_{i_j})}$$
Thus there are at most $3^d$ locally optimal solutions. $d$ is usually small, even when the rule tree $U$ itself is big, and so we can enumerate all $3^d$ locally optimal solutions and find their sampling cost and probability scores. 

\begin{comment}
Let $r_0 \in U \setminus L$. Let $d$ be the number of leaf children of $r_0$. Let the children be $r_1, r_2, ... r_d$, ordered such that $S(r_0, r_i) > S(r_0, r_j) \forall i > j$. Then we can show that for any locally optimal solution, there exist integers $i_m, i_M$ such that the following conditions hold:
$$1 \leq i_m \leq i_M \leq d$$
$$n_{r_0} = \frac{minSS}{S(r_0, r_{i_m})}$$
$$1 \leq j \leq i_m \Rightarrow n_{r_j} = 0$$
$$i_m < j \leq i_M \Leftrightarrow n_{r_j} = minSS - \frac{minSS}{S(r_0, r_{i_j})}$$
$$i_M < j \Rightarrow n_{r_j} = 0$$
And thus
$$ess(r_j) \geq minSS \Leftrightarrow j \leq i_M$$

Call this locally optimal solution $e$. Then we define its `probability value' to be
$$P(e) = \sum_{j = 1}^{i_M}p_{j}$$
and its `Sampling Cost' to be
$$S(e) = \frac{minSS}{S(r_0, r_{i_m})} + \sum_{j=i_m+1}^{i_M} minSS - \frac{minSS}{S(r_0, r_{i_j})}$$
Since there are at most $d^2$ possible values of $i_m$ and $i_M$, we can try out all of them and obtain all locally optimal solutions. 
\end{comment}

Then next step is to combine the solutions using dynamic programming. Let the $M$ sets be called $M_0, M_1, ... M_D$. Let our possible sample sizes range from $0$ to $\mathcal{S}$. The number of sample sizes can be pretty large ($\mathcal{S}$), but we can make it smaller by discretizing the sample sizes, say to have granularity $100$. Then we create a $D \times \mathcal{S}$ array $A$. The value $A\left[i\right]\left[j\right]$ contains the best probability score we can get from $M_0, M_1, ... M_i$ with total sample size at most $j$. We can populate $A\left[0\right]\left[j\right] \forall j$ using the locally optimal solutions for $M_0$. Let $E_{i+1}$ denote the set of locally optimal solutions for $M_{i+1}$. Then we have,
$$A\left[i+1\right] \left[j \right] = \textrm{max} (A\left[i\right]\left[j\right], \textrm{max}_{e \in E_{i+1}}(A\left[i\right]\left[j-S(e)\right] + P(e)))$$
This can be solved using dynamic programming, in $O(D\mathcal{S}d^2)$ time. 


\subsubsection{Additional optimizations}
There are some additional minor optimizations we can make to reduce the memory cost per sample, allowing us to store more and bigger samples. 
Suppose we have a sample $s$, and say its filter rule $f_s$ has value $v$ in column $c$. Then we know that each tuple $t$ in $T_s$ must also have value $v$ in column $c$, since it is covered by $f_s$. So we do not need to explicitly store the column $c$ value of any tuple in $T_s$. We only need to store the tuple values of columns that have a $\star$ value in $f_s$.
In addition, we may have a tuple occur in multiple samples. Instead of storing the entire tuple repeatedly, we could create a dictionary of common tuples, and only store a pointer to the tuple's dictionary entry in $T_s$. 

\subsubsection{Setting $minSS$}
Suppose a rule $r$ covers $x$ fraction of the tuples of $T$ i.e. $x|T|$ tuples. Say we have a uniform random sample $s$ of $T$. The samples has size $|T_s|$, and let $X_{r,s}$ be the random variable denoting the number of tuples of $T_s$ covered by $r$. Then $E\left[ X_{r,s} \right] = x|T_s|$, and $\text{Dev}(X_{r,s}) \approx \sqrt{|T_s|x(1-x)}$. In order to get a good estimate of $x$ (and hence of Count$(r) = x|T|$), we want $E\left[X_{r,s}\right] >> \text{Dev}(X_{r,s})$. That is, $x|T_s| >> \sqrt{|T_s|x(1-x)} \Leftrightarrow \frac{x|T_s|}{1-x} >> 1$. 

We want to set the parameter minSS such that we get good count estimates for rules when using a sample of size $|T_s| = minSS$. If a rule displayed in our summary has covers $x$ fraction of the tuples, we want minSS to be at least $\rho\frac{1-x}{x}$, So the value of minSS must be at least $\rho\frac{1-x}{x}$ where $\rho$ is a constant chosen by us based on how accurate we want the count estimate to be. Moreover, since we want good Count estimates for all rules displayed in the summary, we want $minSS >> \rho\frac{1-x}{x}$ where $x$ is the minimum fraction of tuples covered by any of the rules displayed in our summary.

Thus a reasonable value of minSS can be found by obtaining a bound on $\frac{1-x}{x}$. This is hard to do for arbitrary weighting functions, but we can do it for the Size weighting function (where weight of a rule equals number of non-$\star$ values of the rule). Let $c$ be the column with the fewest distinct values. Say it has $|c|$ values. Then the rule that has the most frequent value of $c$, and $\star$ everywhere else, must have a score of at least $\frac{|T|}{|c|}$. For example, if the table has $10000$ tuples in all, and there is a `Education' column that has 5 possible values, then the most frequent value of Education must occur at least $2000$ times. So the rule with the most frequent value for Education, and $\star$s elsewhere, must have a score of at least $2000$. 

The highest scoring rule can have weight at most $|C|$ (the total number of columns). Since the score of the highest scoring rule is at least $\frac{|T|}{|c|}$, the Count of the highest scoring rule must be at least $\frac{|T|}{|C||c|}$. Thus if minSS is significantly larger than $|C||c|$, then the Count of the first few highest scoring rules should be well-approximated in a sample of size more than minSS. For example, if $|T| = 10000$, $|c| = 5$, $|C| = 10$, then we want $minSS >> 5 \times 10$.
