%!TEX root = TableSummarization.tex


\section{Smart drill down Algorithms} \label{sec:algorithms}
We now describe online algorithms for implementing
the smart drill down operator. We assume in this section that all columns are categorical (so numerical columns have been bucketized beforehand). \papertext{We further discuss bucketization of numerical attributes in the Extensions section in the technical report~\cite{tr}}\techreporttext{We further discuss bucketization of numerical attributes in Section~\ref{sec:extensions}}.


\subsection{Problem Reduction and Important Property} \label{sec:reduction}
When the user drills down on a rule $r^{\prime}$, we want to find the highest scoring list of rules to expand rule $r^{\prime}$ into. If the user had clicked on a $\star$ in a column $c$, then we have the additional restriction that all resulting rules must have a non-$\star$ value in column $c$. We can reduce Problem~\ref{prob:optimal-subrule-list} to the following simpler problem by removing the user-interaction based constraints: 

\begin{problem}\label{prob:optimal-rule-list}
Given a table $T$, a monotonic weight function $W$, and a number $k$, to find the list $R$ of $k$ rules that maximizes the total score given by :
$$\text{Score}(R) = \sum_{r \in R}W(r)MCount(r,R)$$
\end{problem}

\noindent Problem~\ref{prob:optimal-subrule-list} with parameters $(T, W, k)$ can be reduced to Problem~\ref{prob:optimal-rule-list} as follows:
\begin{enumerate}
\item $[$Rule drill down$]$ If the user clicked on rule $r$ in Problem~\ref{prob:optimal-subrule-list}, then we can conceptually make one pass through the table $T$ to filter for tuples covered by rule $r$, and store them in a temporary table $T_r$. Then, we solve Problem~\ref{prob:optimal-rule-list} for parameters $(T_r, W, k)$.
\item $[$Star drill down$]$ If the user clicked on a $\star$ in column $c$ of rule $r$, then we first filter table $T$ to get a smaller table $T_r$ consisting of tuples from $T$ that are covered by $r$. In addition, we change the weight function $W$ from Problem~\ref{prob:optimal-subrule-list} to a weight function $W^{\prime}$ such that : For any rule $r^{\prime}$, $W^{\prime}(r^{\prime}) = 0$ if $r^{\prime}$ has a $\star$ in column $c$, and $W^{\prime}(r^{\prime}) = W(r^{\prime})$ otherwise. Then, we solve Problem~\ref{prob:optimal-rule-list} for parameters $(T_r, W^{\prime}, k)$.
\end{enumerate}

\begin{algorithm}
\scriptsize
\KwIn{$k$ (Number of rules required), $T$ (database table), $m_w$ (max weight), $W$ (weight function)}
\KwOut{$S$ (Solution set of rules)}
$S = \phi$ 

\For {$i$ from $1$ to $k$}{
$R_m = \text{Find\_best\_marginal\_rule}(S, T, m_w, W)$ \tcc*{Calling Algorithm~\ref{algo:best-marginal-rule}}

$S = S \cup \left\lbrace R_m \right\rbrace$
}
\Return $S$
\caption{Greedy Algorithm for Problem~\ref{prob:optimal-rule-set}\label{algo:best-rule-set}}
\end{algorithm}

As a first step towards solving Problem~\ref{prob:optimal-rule-list}, we show that the rules in the optimal list must effectively be ordered in decreasing order by weight. Note that the weight of a rule is independent of its $MCount$. The $MCount$ of a rule is the number of tuples that have been `assigned' to it, and each tuple assigned to rule $r$ contributes $W(r)$ to the total score. Thus, if the rules are not in decreasing order by weight in a rule list $R$, then switching the order of rules in $R$ transfers some tuples from a lower weight rule to a higher weight rule, which can increase total score.

\begin{lemma}\label{lemma:rule-ordering}
Let $R$ be a rule-list. Let $R^{\prime}$ be the rule-list having the same rules as $R$, but ordered in descending order by weight. Then
$\text{Score}(R^{\prime}) \geq \text{Score}(R)$.
\end{lemma}
\papertext{The proof of this lemma, as well as other proofs, can be found in the appendix of the technical report~\cite{tr}}\techreporttext{The proof of this lemma, as well as other proofs, can be found in the appendix}. 
Thus, it is sufficient to restrict our attention to rule-lists that have rules sorted in decreasing order by weight. Or equivalently, we can define Score for a \emph{set} of rules as follows:

\begin{definition}\label{def:set-score}
Let $R$ be a set of rules. Then the Score of $R$ is
$\text{Score}(R) = \text{Score}(R^{\prime})$
where $R^{\prime}$ is the list of rules obtained by ordering the rules in the set $R$ in decreasing order by weight.
\end{definition}

This gives us a reduced version of Problem~\ref{prob:optimal-rule-list}: 
\begin{problem}\label{prob:optimal-rule-set}
Given a table $T$, a monotonic weight function $W$, and a number $k$, find the set (not list) $R$ of $k$ rules which maximizes Score($R$) as defined in Definition~\ref{def:set-score}.
\end{problem}

\noindent The reduction from Problem~\ref{prob:optimal-rule-list} to Problem~\ref{prob:optimal-rule-set} is clear. We now first show that Problem~\ref{prob:optimal-rule-set}, and consequently Problem~\ref{prob:optimal-subrule-list} and Problem~\ref{prob:optimal-rule-list} are {\sc NP-Hard}, and then present an approximation algorithm for solving Problem~\ref{prob:optimal-rule-set}.

\subsection{NP-Hardness for Problem~\ref{prob:optimal-rule-set}}
We reduce the well known {\sc NP-Hard} {\em Maximum Coverage Problem} to a special case of Problem~\ref{prob:optimal-rule-set};
thus demonstrating the {\sc NP-Hard}ness of Problem~\ref{prob:optimal-rule-set}. The Maximum Coverage Problem is as follows: 
\begin{problem}\label{prob:maximum-coverage}
Given a universe set $U$, an integer $k$, and a set $S = \left\lbrace S_1, S_2, ... S_m \right\rbrace$ of subsets of $U$ (so each $S_i \subset U$), find $S^{\prime} \subset S$ such that $|S^{\prime}| = k$, which maximizes $\text{Coverage}(S^{\prime}) = |\bigcup_{s \in S^{\prime}} s|$.
\end{problem}
Thus, the goal of the maximum coverage problem is to find a set of $k$ of the given subsets of $U$ whose union `covers' as much of $U$ as possible. We can reduce an instance of the Maximum Coverage Problem (with parameters $U, k, S$) to an instance of Problem~\ref{prob:optimal-rule-set}, which gives us the following lemma:
\begin{lemma}
Problem~\ref{prob:optimal-rule-set} is {\sc NP-Hard}.
\end{lemma}


\subsection{Algorithm Overview}\label{sec:alg-overview}
Given that the problem is {\sc NP-Hard}, we now present our algorithms 
for approximating the solution to Problem~\ref{prob:optimal-rule-set}. 
The problem consists of finding a set of rules, given size $k$, that maximizes 
Score. 


The next few sections fully develop the
details of our solution:

\squishlist
\item We show that the Score function is {\em submodular}, and hence an approximately optimal set can be obtained using a greedy algorithm. At a high level, this greedy algorithm is simple to state. The algorithm runs for $k$ steps;
we start with an empty rule set $R$, and then at each step, we add the next best rule that maximizes Score
 
\item In order to find the rule $r$ to add in each step, we need to measure the impact on Score for each $r$. This is done in several passes over the table, using ideas from the a-priori algorithm~\cite{apriori} for frequent item-set mining. 
\squishend

\begin{algorithm}
\scriptsize
\KwIn{$S$ (Current solution set), $T$ (database table), $m_w$ (max weight), $W$ (weight function)}
\KwOut{$R_m$ (Rule which adds the highest marginal value among rules with weight $\leq m_w$)}
$H = 0$ \tcc*{Threshold for deciding if to count for a rule.}
$C = C_o = C_n = \phi$ \tcc*{Set of all, old and new candidate rules respectively. }
\For{$j$ from $1$ to number of columns in $T$}{
\If {$j = 1$}{
$C_n = \text{ all rules of size } 1$
}
\Else{ 
$C_n = \text{all size-}i$ super-rules of rules from $C_o$
}
\ForEach {$R \in C_n$}{
$M =\infty$ \tcc*{Upper bound on marginal value count of $R$}
\ForEach {$R$-sub-rule $R^{\prime} \in C$}{  
$M = \text{min}(M, \text{MarginalVal}(R^{\prime}) + \text{Count}(R^{\prime})(m_w - W(R^{\prime}))$
}
\If {$(M < H)$}{ 
$C_n = C_n \setminus \left\lbrace R \right\rbrace$ \tcc{Delete $R$ if its max count is too small for $R$ to be in the solution}
}
}
\If {$C_n = \phi$}{
break;
}
\ForEach {$R \in C_n$}{
Count$(R) = 0$ \tcc*{Initialize}
MarginalValue$(R) = 0$ \tcc*{Initialize}
}
\ForEach {$t \in T$}{
Let $R_S$ be the highest weight rule in $S$ that covers $t$

\ForEach {$R \in C_n$ that covers $t$}{
Count($R$) $++$ 

MarginalValue($R$) $+=$ $W(R) - \text{min}(W(R), W(R_S))$
}
}
$C = C \cup C_n$

$C_o = C_n$

$C_n = \phi$


$H = \textrm{max}_{R \in C}(\text{MarginalValue}(R))$
}
\Return $\textrm{argmax}_{r \in C} \text{MarginalValue}(r)$
\caption{Find best marginal rule\label{algo:best-marginal-rule}}
\end{algorithm}

In some cases, the dataset may still be too large for us to return a good rule set in
a reasonable time; in such cases, we may want to run our algorithm on a sample of the table
rather than the entire table. In Section~\ref{sec:sampling}, we describe a scheme 
for maintaining multiple samples in memory and using them to improve response 
time for different drill down operations performed by the user. Our sampling scheme dynamically adapts to the current interaction scenario that the user is in; drawing from ideas in approximation algorithms and optimization theory.

\subsection{Greedy Approximation Algorithm}\label{sec:greedy-approx}
\stitle{Submodularity:} We will now show that the Score function over sets of rules has a property called {\em submodularity}, giving us a greedy approximation algorithm for optimizing it. 
\begin{definition}
A function $f: 2^S \rightarrow \mathbb{R}$ for any set $S$ is said to be submodular if and only if, for every $s \in S$, and $A \subset B \subset S$ with $s \notin A$: $f(A \cup \left\lbrace s \right\rbrace) - f(A) \geq f(B \cup \left\lbrace s \right\rbrace) - f(B)$
\end{definition}
Intuitively, this means that the marginal value of adding an element to a set $S$ cannot increase if we add it to a superset of $S$ instead. For monotonic non-negative submodular functions, it is well known that the solution to the problem of finding the set of a given size with maximum value for the function can be found approximately in a greedy fashion. 

\begin{lemma}\label{lemma:submodular}
For a given table $T$, the Score function over sets $S$ of rules, defined by the following is submodular:
$$\text{Score}(S) = \sum_{r \in S} MCount(r,S)W(r)$$
\end{lemma}
\vspace{-10pt}

\stitle{High-Level Procedure:} Now, based on the submodularity property, the greedy procedure,
as listed below, has desirable approximation guarantees:
\vspace{-5pt}
\begin{framed}
\vspace{-5pt}
\begin{denselist}
\item Set $S = \phi$
\item For $i$ from $1$ to $k$
\begin{denselist}
\item Find the rule $r$ for which Score($S \cup \left\lbrace r \right\rbrace$) is the highest.
\item $S = S \cup \left\lbrace r \right\rbrace$
\end{denselist}
\end{denselist}
\vspace{-5pt}
\end{framed}
\vspace{-5pt}
Since Score is a submodular function of the set $S$, 
this greedy procedure is guaranteed to give us a score within a $1 - \frac{1}{e}$ factor of the optimum. 

The expensive step in the above procedure is the step where the Score is computed for every
single rule. Given the number of rules can be large, this can be an especially time-consuming process.

Instead of using the procedure described above directly, we instead develop a ``parameterized'' version 
that will admit further approximation (depending on the parameter) in order to reduce computation further. We describe
this algorithm next.

\stitle{Parametrized Algorithm:} Our algorithm pseudo-code is given in the box labelled Algorithm~\ref{algo:best-rule-set}. We call our algorithm {\em BRS} (for \textbf{B}est \textbf{R}ule \textbf{S}et). BRS takes four parameters as input: the table $T$, the number $k$ of rules required in the final solution list, a parameter $m_w$ (which we describe in the next paragraph), and the weight function $W$. 

The parameter $m_w$ stands for \textit{Max Weight}. The parameter $m_w$ tells the algorithm to assume that all rules that get selected in the optimal solution are going to have weight $\leq m_w$. Thus, if $S_o$ denotes set of rules with maximum score, then as long as $m_w \geq \textrm{max}_{r \in S_o}W(r)$, BRS is guaranteed to return $S_o$. On the other hand if $m_w < W(r)$ for some $r \in S_o$, then there is a chance that the set returned by BRS does not contain $r$. BRS runs faster for smaller values of $m_w$, and may only return a suboptimal result if $m_w < \textrm{max}_{r \in S_o}W(r)$. In practice, $\textrm{max}_{r \in S_o}W(r)$ is usually small. This is because as the size (and weight) of a rule increases, its Count falls rapidly. The Count tends to decrease exponentially with rule size, while Weight increases linearly for common weight functions (such as $W(r) = \text{Size}(r)$). Thus, rules with high weight and size have very low count, and are unlikely to occur in the optimal solution set $S_o$. Our experiments in Section~\ref{sec:experiments} also show that the weights of rules in the optimal set tend to be small.

% TODO: scheme/heuristic for finding good m_w values, since it seems arbitrary. Later, experiments testing effect of m_w on running time, and experiments to test finding heuristic, and show that values are in fact small.

BRS initializes the solution set $S$ to be empty, and then iterates for $k$ steps, adding the best marginal rule at each step. To find the best marginal rule, it calls a function to find the best marginal rule given the existing set of rules $S$. 

\stitle{Finding the Best Marginal Rule:} In order to find the best marginal rule, we need to find the marginal values of several rules and then choose the best one. A brute-force way to do this would be to enumerate all possible rules, and to find the marginal value for each of those rules in a single pass over the data. But the number of possible rules may be almost as large as the size of the table itself, making this step very expensive in terms of computation and memory. 

In order to avoid counting too many rules, we leverage a technique inspired by the {\em a-priori} algorithm for frequent itemset mining~\cite{apriori}. Recall that the a-priori algorithm is used to find all frequent itemsets that have a support greater than a threshold. Unlike the a-priori algorithm, our goal is to find the single best marginal rule. Since we only aim to find one rule at a time, our pruning power is significantly higher than a vanilla a-priori algorithm, and we terminate in much fewer passes over the dataset. 

We compute the best marginal rule over multiple passes on the dataset, with the maximum number of passes equal to the maximum size of a rule. In the $j^{th}$ pass, we compute counts and marginal values for rules of size $j$. To give an example, suppose we had three columns $c_1$, $c_2$, and $c_3$. In the first pass, we would compute the counts and marginal values of all rules of size $1$. In the second pass, instead of finding marginal values for all size $2$ rules, we can use our knowledge of counts from the first pass to upper bound the potential counts and marginal values of size $2$ rules, and be more selective about which rules to count in the second pass. For instance, suppose we know that the rule $(a, \star, \star)$ has a count of $1000$, while $(\star, b, \star)$ has a count of $100$. Then for any value $c$ in column $c_3$ we would know that the count of $(\star, b, c)$ is at most $100$ because it cannot exceed that of $(\star, b, \star)$. This implies that the maximum marginal value of any super-rule of $(\star, b, c)$ having weight $\leq m_w$ is at most $100m_w$. If the rule $(a, \star, \star)$ has a marginal value of $800$, then the marginal value of any super-rule of $(\star, b, \star)$ cannot possibly exceed that of $(a, \star, \star)$. Since our aim is to only find the highest marginal value rule, we can skip counting for all super-rules of $(\star, b, \star)$ for future passes.

We now describe the function to find the best marginal rule. The pseudo-code for the function is in the box titled Algorithm~\ref{algo:best-marginal-rule}. The function maintains a threshold $H$, which is the highest marginal value that has been found for any rule so far. The function makes several iterations (Step $3$), counting marginal values for size $j$ rules in the $j^{th}$ iteration. We maintain three sets of rules. $C$ is the set of all rules whose marginal values have been counted in all previous iterations. $C_n$ is the set of rules whose marginal values are going to be counted in the current pass. And $C_o$ is the set of rules whose marginal values were counted in the previous iteration. For the first pass, we set $C_n$ to be all rules of size $1$. Then we compute marginal values for those rules, and set $C = C_o = C_n$.

For the second pass onwards, we are more selective about which rules to consider for marginal value evaluation. We first set $C_n$ to be the set of rules of size $j$ which are super-rules of rules from $C_o$. Then for each rule $r$ from $C_n$, we consider the known marginal values of its sub-rules from $C$, and use them to upper-bound the marginal value of all super-rules of $r$, as shown in Step 3.3.2. Then we delete from $C_n$ the rules whose marginal value upper bound is less than the currently known best marginal value, since they have no chance of being returned as the best marginal rule. Then we make as actual pass through the table to compute the marginal value of the rules in $C_n$, as shown in Step 3.5. If in any round, the $C_n$ obtained after deleting rules is empty, then we terminate the algorithm and return the highest value rule. 

The reader may be wondering why we did not simply count the score of each rule using a variant of the a-priori algorithm in one pass, and then pick the set of rules that maximizes score subsequently. This is because doing so will lead to a sub-optimal set of rules: by not accounting for the rules that have already been selected, we will not be able to ascertain the marginal benefit of adding an additional rule correctly.




\input{sampling}