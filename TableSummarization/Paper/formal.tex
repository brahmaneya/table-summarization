%!TEX root = TableSummarization.tex


\section{Formal Description}\label{sec:formal}
We describe our formal problem
in Section~\ref{sec:preliminaries},
describe different scoring functions
in Section~\ref{sec:weighting},
and describe our operator interfaces
in Section~\ref{sec:interface}. 
\subsection{Preliminaries and Definitions}
\label{sec:preliminaries}

\stitle{Tables and Rules:} As in a traditional OLAP setting, we assume we are given a 
star or snowflake schema;
for simplicity, we represent this schema using a single denormalized relational table,
which we call $\calD$. 
For the purpose of the rest of the discussion, we will operate on this
table $\calD$.
We let $T$ denote the set of tuples in $\calD$, and $C$ denote 
the set of columns in $\calD$.

Our objective (formally defined later) is to 
enable smart drill-downs on this table or on portions of it:
the result of our drill-downs are lists of {\em rules}. 
A {\em rule} is a tuple with a value for each column of the table. 
In addition, a rule has other attributes, such as count and weight 
(which we define later) associated with it. 
The value in each column of the rule can either be one of the values in the corresponding column of the table, or $\star$, representing a wildcard character representing all values in the column. For a column with numerical values in the table, we allow the corresponding rule-value to be a range instead of a single value. The {\em trivial rule} is one that has a $\star$ value in all columns. The {\em Size} of a rule is defined as the number of non-starred values in that rule.

\stitle{Coverage:} A rule $r$ is said to {\em cover} a tuple  $t$ from the table if all non-$\star$ values for all columns of the rule match the corresponding values in the tuple. We abuse notation to write this as $t \in r$. At a high level, we are interested
in identifying rules that cover many tuples. We next define the concept of subsumption that allow us to 
relate the coverage of different rules to each other.

We say that rule $r_1$ {\em subsumes} rule $r_2$ if and only if $r_1$ has no more stars than $r_2$ and their values match wherever they both have non-starred values. For example, rule ($a$, $\star$) subsumes rule ($a$, $b$). Thus if $r_1$ subsumes $r_2$, then for all tuples $t$, $t \in r_2 \Rightarrow t \in r_1$. We also write $r_1 \geq r_2$ to denote that $r_1$ subsumes $r_2$, and $r_1 > r_2$ if $r_1$ subsumes but is not equal to $r_2$ i.e. there is at least one column $c$ such that $r_2$ has a non-$\star$ value in $c$ where $r_1$ does not. If $r_1$ subsumes $r_2$, we also say that $r_1$ is a {\em sub-rule} of $r_2$ and $r_2$ is a {\em super-rule} or $r_1$.

\stitle{Rule Lists:}
A {\em rule-list} is an ordered list of rules returned by our system on use of a smart drill-down operator. 
When a user drills down on a rule $r$ to know more about the part of the table covered by $r$, we display a rule-list of sub-rules of $r$.
For instance, the second, third and fourth rule from Table~\ref{table:introexample} form a rule-list, which is displayed when the user clicks on the first (trivial) rule. Similarly, the second, third and fourth rules in Table~\ref{table:introexample2} form a rule-list, as do the fifth, sixth and seventh rules. 

\stitle{Scoring:} We now define some additional properties of rules; these properties
help us ``score'' individual rules as part of a rule-list. 

There are two portions that constitute our scores for a rule as part of a rule list. 
The first portion dictates how much the rule $r$ ``covers'' the tuples in $\calD$;
the second portion dictates how ``good'' the rule $r$ is (independent of how many
tuples it covers). 
The reason why we separate the scoring into these two portions is
that they allow us to separate the inherent goodness of a rule from
how much it captures the data in $\calD$.

We now describe the first portion:
we define {\em Count}($r$) as the total number of tuples $t \in T$ that are covered by $r$. 
Further, we define {\em MCount}($r, R$) (which stands for `Marginal Count') as the number of tuples covered by $r$ but not by any rule before $r$ in the rule-list $R$. A high value of $MCount$ indicates that the rule not only covers a lot of tuples, but also covers parts of the table not covered by previous rules. We want to pick rules with a high value of $MCount$ to display to the user
as part of the smart drill-down result, to increase the coverage of the rule-list. 

Now, onto the second portion: we let $W$ denote a function that assigns a non-negative {\em weight} to a rule based on how good the rule is, with higher weights assigned to better rules. 
As we will see, the weighting function does not depend on the specific
tuples in $\calD$, but could, as we will see later, 
depend on the number of $\star$s in $r$,
the schema of $\calD$, 
as well as the number of distinct values in each column of $\calD$.
A weighting function is said to be {\em monotonic} if for all rules $r_1$, $r_2$ such that $r_1$ is a sub-rule of $r_2$, we have $W(r_1) \leq W(r_2)$; we focus
on monotonic weighting functions because we prefer 
rules that are more ``specific''
rather than those that are more ``general'' 
(thereby conveying less information).
We further describe our weighting functions in Section~\ref{sec:weighting}. 

Thus, the total score for our list of rules is given by 
$$\text{Score}(R) = \sum_{r \in R} \underbrace{MCount(r, R)}_{\text{coverage of $r$ in $\calD$}} \times \underbrace{W(r)}_{\text{weight of $r$}}$$ 
Overall, our goal is to choose the rule-list that maximizes the 
total score. 


We use $MCount$ rather than Count in the above equation to ensure that we do not redundantly cover
the same tuples multiple times using multiple rules, and thereby increase coverage of the table. 
If we had defined total score as $\sum_{r \in R} \text{Count}(r)W(r)$, then our optimal rule-list could 
contain rules that repeatedly refer to the most `summarizable' part of the table. 
For instance, if $a$ and $b$ were the most common values in columns $A$ and $B$, then 
for some weighting functions $W$, 
the summary may potentially consist of rules $(a, b, \star)$, $(a, \star, \star)$, and $(\star, b, \star)$, which tells us nothing about the part of the table with values other than $a$ and $b$. 

Our smart drill downs still display the Count of each rule rather than the $MCount$. This is because while $MCount$ is useful in the rule selection process, Count is easier for a user to interpret. In any case, it would be a simple extension to display MCount in another column.

\stitle{Formal Problem:} We now formally define our problem:
\begin{problem}\label{prob:optimal-subrule-list}
Given a table $T$, a monotonic weighting function $W$, and a number $k$, find the list $R$ of $k$ rules that maximizes 
$$\sum_{r \in R} W(r) \times MCount(r,R)$$
for one of the following smart drill-down operations:
\begin{enumerate}
\item $[$Rule Drill-Down$]$ If the user clicked on a rule $r^{\prime}$, then all $r \in R$ must be super-rules of $r^{\prime}$
\item $[$Star Drill-Down$]$ If the user clicked on a $\star$ on column $c$ of rule $r^{\prime}$, then all $r \in R$ must be super-rules of $r^{\prime}$ and have a non-$\star$ value in column $c$
\end{enumerate}
\end{problem}
Throughout this paper, we use the {\em Count} aggregate of a rule to display to the user. We can also use a {\em Sum} of values over a given `measure column' $c_m$ instead. We discuss how to modify our algorithms to use $Sum$ instead of $Count$ in the `Extensions' section of the technical report~\cite{tr}.

\subsection{Weighting Rules}
\label{sec:weighting}
We now describe our weighting function $W$ that is used to score individual rules. 
At a high level, we want our rules to be as descriptive of the table as possible, i.e. given the rules, it should be as easy as possible to reproduce the table. We consider a general family of weighting functions, that assigns for each rule $r$, a weight $W(r)$ depending on how expressive the rule is (i.e., how much information
it conveys). Let us consider some canonical forms for the function $W(r)$ and their interpretation; later, we describe the full
space of weighting functions our techniques can handle:

\stitle{Size Weighting Function:} 
$W(r) = |\left\lbrace c \in C \mid r(c) \neq \star \right\rbrace |$ : 
Here we set weight equal to the number of non-starred values in the rule $r$; we define this quantity to be the {\em size} of the rule. Consider the examples in Table \ref{table:sizescoringexample}. The weight for rule ($a$, $b_1$) is $2$, while weight for ($a$, $\star$) is $1$. Thus the total score for the rule-list with these two rules would be $2 \times 100 + 1 \times 900 = 1100$. If we replaced the rule ($a$, $\star$) by two rules ($a$, $b_2$) and ($a$, $\star$), then the $MCount$ of rule ($a$, $\star$) would reduce to $600$, since $300$ of its tuples would be covered by the previous rule $(a, b_2)$. Thus our total score would be $2 \times 100 + 2 \times 300 + 1 \times 600 = 1400$. If we had instead replaced ($a$, $\star$) by ($a$, $b_3$) and ($a$, $\star$), then our score would have been $1500$ which is $> 1400$. Thus when we marginalize on one extra column ($B$ in this case) and include a rule where that column is instantiated, it is better to do so on the value which occurs more frequently (in this case, $b_3$ which occurred $400$ times, compared to the $300$ of $b_2$). 

To get an intuitive feel for this scoring function, imagine we are trying to reconstruct the table from the rules. Since we have rule ($a$, $b_1$) with $MCount$ $100$, we are going to get a $100$ of the table's tuples from this rule. For those hundred tuples, out of the $200$ total values to be filled ($2$ per tuple, since there are $2$ columns), all $200$ values will already have been filled (since the rule specifies both columns). Thus this rule contributes $200$ to the score. For the rule ($a$, $\star$), there are $900$ table tuples, and the $a$ value will be pre-filled for those tuples. Thus $900$ slots of these tuples have been pre-filled, and so the rule contributes $900$ to the total. Thus this scoring function can be thought of as the number of values that have been pre-filled in the table by our rule-list. Since having more of the table pre-filled is better, maximizing the score gives us a desirable set of rules.

\stitle{Bits Weighting Function:}
$W(r) = \sum_{c \in C : r(c) \neq \star} \lceil \text{log}_2(|c|) \rceil$ where $|c|$ refers to the number of distinct possible values in column $c$. Like the previous scoring function, this one adds some weight for every non-starred value in a rule. But instead of adding a weight of $1$ for every non-starred value, it varies the weight added depending on the inherent complexity of the column. The reason behind this variation is the following: Say column $c_1$ is a boolean, while $c_2$ is a column with $20$ possible values. Then, a rule that gives us a value for $c_2$ is clearly giving us more information than a rule that gives us a value for $c_1$. Thus, this scoring function gives a higher weight to a rule that gives a value for a column with more distinct values. 

The interpretation for this function is similar to the one for the last function. Once again, we can imagine trying to reconstruct the table from the rules, and look at how much of the table is pre-filled by the rules. But this time, we count the number of `bits' that are pre-filled. For a column $c$, specifying a value in the column takes $\lceil \text{log}(|c|)\rceil$ bits of information. A non-starred valued in a rule $r$ thus pre-fills $MCount(r,R) \lceil \text{log}(|c|) \rceil$ bits of the table. Hence this scoring function gives us the the number of bits of the table that are pre-filled by the rule set.

Note that this scoring function is closely related to the Minimum Description Length (MDL)~\cite{Grunwald:2007:MDL:1213810} of a table. If we describe a table using the rule-set, plus values to fill in for $\star$s in the rules to get tuples, then finding a set of rules of given size $k$ that tries to minimize the length of this description, is equivalent to finding the rule-set that maximizes the total score. 

\smallskip

\stitle{Other Weighting Functions:}
Even though we have given two example weighting functions here, our algorithms allow the user to leverage any weighting function $W$, subject to only two conditions:
\squishlist
\item Non-negativity: For all rules $r$, $W(r) \geq 0$.
\item Monotonicity: If $r_1 \geq r_2$, then $W(r_1) \leq W(r_2)$. Monotonicity means that a rule that is less descriptive than another must be assigned a lower weight.
\squishend
A weight function can be used in several ways, including expressing a higher preference for a column (by assigning higher weight to rules having a non-$\star$ value in that column), or expressing indifference towards a column (by adding zero weight for having non-$\star$ value in that column).

\begin{table}
\centering
\scriptsize
\begin{tabular}{ | l | c | }
 \hline Rule-MCount list & Score \\ \hline
  ($a$, $b_1$)-$100$, ($a$, $\star$)-$900$ & $1100$ \\
  ($a$, $b_1$)-$100$, ($a$, $b_2$)-$300$, ($a$, $\star$)-$600$ & $1400$  \\
  ($a$, $b_1$)-$100$, ($a$, $b_3$)-$400$, ($a$, $\star$)-$500$ & $1500$ \\ \hline
\end{tabular}
\vspace{-10pt}
\caption{Example of Rule-based scoring with score equal to rule size \label{table:sizescoringexample}}
\vspace{-15pt}
\end{table}

\subsection{Smart Drill-Down Operations}
\label{sec:interface}
When the user starts using a system equipped with
the smart drill-down operator, they first see a table with a single trivial rule as shown in table~\ref{table:introexample0}. At any point, the user can click on either a rule, or a star within a rule, to perform a `smart drill-down' on the rule. Clicking on a rule $r$ causes $r$ to expand into the highest-scoring rule-list consisting of super-rules of $r$. By default, the rule $r$ expands into a list of $3$ rules, but this number can be changed by the user. As an example of this operation, clicking on the trivial rule of table~\ref{table:introexample0} would display table~\ref{table:introexample}. Clicking further on the third rule in the expanded rule-list would display table~\ref{table:introexample2}. The rules obtained from the expansion are listed directly below $r$, ordered in decreasing order by weight (the reasoning behind the ordering is explained in Section~\ref{sec:algorithms}).

Instead of clicking on a rule, the user can click on one of the $\star$s, say in column $c$ of rule $r$. This will also cause the rule $r$ to expand into a rule-list, but this time the new displayed rules are guaranteed to have non-$\star$ values for in column $c$. For instance, if the user clicks on the $\star$ in the product column of the walmart rule, they will see table~\ref{table:introexample3}, which shows super-rules of the walmart rule all specific to some product. This operation is useful if the user is more interested in a particular column that is unlikely to be instantiated in the top rules otherwise. 

Finally, when the user clicks on a rule that has already been expanded, it reverses the expansion operation, i.e. collapses it. For example, clicking on the walmart rule in table~\ref{table:introexample3} or table~\ref{table:introexample2} would take the user back to table~\ref{table:introexample}. This operation is equivalent to a traditional
roll-up, but for smart drill-downs instead of traditional drill-downs.
	
\begin{table}
\scriptsize
\centering
\begin{tabular}{| l | l | l | l | l |}
\hline Store & Product & State & Count & Weight \\
\hline
$\star$ & $\star$ & $\star$ & $6000$ & $0$ \\ \cline{1-5}
$\triangleright$ target & bicycles & $\star$ & $200$ & $2$ \\ \cline{1-5}
$\triangleright$ $\star$ & comforters & Massachusetts & $600$ & $2$ \\ \cline{1-5}
$\triangleright$ walmart & $\star$ & $\star$ & $1000$ & $1$ \\ \cline{2-5}
$\triangleright$ $\triangleright$ walmart & cookies & California & $80$ & $2$ \\ \cline{2-5}
$\triangleright$ $\triangleright$ walmart & cookies & $\star$ & $200$ & $2$ \\ \cline{2-5}
$\triangleright$ $\triangleright$ walmart & bicycles & $\star$ & $150$ & $2$ \\  \hline
\end{tabular}
\vspace{-10pt}
\caption{Result of clicking on a $\star$ \label{table:introexample3}}
\vspace{-10pt}
\end{table}

